{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4006989a-fb7a-409d-b9db-aee25722a654",
   "metadata": {},
   "source": [
    "![](pics/header.jpg)\n",
    "\n",
    "# PyTorch\n",
    "\n",
    "Kevin Walchko, Phd\n",
    "\n",
    "---\n",
    "\n",
    "Some of this material comes from Udacities AI course.\n",
    "\n",
    "## Origins\n",
    "\n",
    "PyTorch was released in early 2017 and has been making a pretty big impact in the deep learning community. It's developed as an open source project by the Facebook AI Research team.\n",
    "\n",
    "- tensor: main data structure \n",
    "- autograd: automatically calculates gradients for backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e255f6-f912-4f9e-8ff6-a9c1e2ff76c4",
   "metadata": {},
   "source": [
    "## Simple Network\n",
    "\n",
    "There are several ways to do this, here is a simple, handcrafted way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aabeb3bc-2fd8-4390-a072-bb761a94eb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchinfo import summary\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "49671e6a-41ba-4819-a746-5394b7887ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Inputs to hidden layer linear transformation\n",
    "        # MNIST images 1 channel (grayscale) x 28 pix x 28 pix\n",
    "        self.hidden = nn.Linear(1*28*28, 256)\n",
    "        # Output layer, 10 units - one for each digit\n",
    "        self.output = nn.Linear(256, 10)\n",
    "        \n",
    "        # Define sigmoid activation and softmax output \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass the input tensor through each of our operations\n",
    "        x = self.hidden(x)  # input to hidden layer \n",
    "        x = self.sigmoid(x) # activation - sigmoid\n",
    "        x = self.output(x)  # hidden to output layer\n",
    "        x = self.softmax(x) # activation - softmax\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830d9e0d-fb60-4449-b15d-2f36c1427564",
   "metadata": {},
   "source": [
    "Let's go through this bit by bit.\n",
    "\n",
    "```python\n",
    "class Network(nn.Module):\n",
    "```\n",
    "\n",
    "Here we're inheriting from `nn.Module`. Combined with `super().__init__()` this creates a class that tracks the architecture and provides a lot of useful methods and attributes. It is mandatory to inherit from `nn.Module` when you're creating a class for your network. The name of the class itself can be anything.\n",
    "\n",
    "```python\n",
    "self.hidden = nn.Linear(784, 256)\n",
    "```\n",
    "\n",
    "This line creates a module for a linear transformation, $x\\mathbf{W} + b$, with 784 inputs and 256 outputs and assigns it to `self.hidden`. The module automatically creates the weight and bias tensors which we'll use in the `forward` method. You can access the weight and bias tensors once the network (`net`) is created with `net.hidden.weight` and `net.hidden.bias`.\n",
    "\n",
    "```python\n",
    "self.output = nn.Linear(256, 10)\n",
    "```\n",
    "\n",
    "Similarly, this creates another linear transformation with 256 inputs and 10 outputs.\n",
    "\n",
    "```python\n",
    "self.sigmoid = nn.Sigmoid()\n",
    "self.softmax = nn.Softmax(dim=1)\n",
    "```\n",
    "\n",
    "Here I defined operations for the sigmoid activation and softmax output. Setting `dim=1` in `nn.Softmax(dim=1)` calculates softmax across the columns.\n",
    "\n",
    "```python\n",
    "def forward(self, x):\n",
    "```\n",
    "\n",
    "PyTorch networks created with `nn.Module` must have a `forward` method defined. It takes in a tensor `x` and passes it through the operations you defined in the `__init__` method.\n",
    "\n",
    "```python\n",
    "x = self.hidden(x)\n",
    "x = self.sigmoid(x)\n",
    "x = self.output(x)\n",
    "x = self.softmax(x)\n",
    "```\n",
    "\n",
    "Here the input tensor `x` is passed through each operation and reassigned to `x`. We can see that the input tensor goes through the hidden layer, then a sigmoid function, then the output layer, and finally the softmax function. It doesn't matter what you name the variables here, as long as the inputs and outputs of the operations match the network architecture you want to build. The order in which you define things in the `__init__` method doesn't matter, but you'll need to sequence the operations correctly in the `forward` method.\n",
    "\n",
    "Now we can create a `Network` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b54d179-47e8-4438-b114-e6202ca86808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs * output + bias: 200960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Network                                  --\n",
       "├─Linear: 1-1                            200,960\n",
       "├─Linear: 1-2                            2,570\n",
       "├─Sigmoid: 1-3                           --\n",
       "├─Softmax: 1-4                           --\n",
       "=================================================================\n",
       "Total params: 203,530\n",
       "Trainable params: 203,530\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "print(\"inputs * output + bias:\", 784 * 256 + 256)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "965a5a8b-f25a-47e0-937c-bad0dc88259d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5e5f906e-588d-44b8-868f-fa51e15e1d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=256, bias=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43f90293-a9a9-422e-82de-6694fca50d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0074, -0.0073,  0.0092,  ...,  0.0243,  0.0182, -0.0071],\n",
       "        [-0.0116,  0.0098, -0.0181,  ...,  0.0103, -0.0216,  0.0062],\n",
       "        [ 0.0109, -0.0217, -0.0352,  ..., -0.0015,  0.0288, -0.0004],\n",
       "        ...,\n",
       "        [ 0.0163, -0.0220,  0.0231,  ..., -0.0241, -0.0234,  0.0254],\n",
       "        [ 0.0226, -0.0325,  0.0094,  ..., -0.0154,  0.0098, -0.0151],\n",
       "        [ 0.0251,  0.0186,  0.0357,  ...,  0.0134, -0.0026, -0.0175]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.hidden.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3322986c-28d3-465d-aee6-c96eb378a7bf",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Typically it's more convenient to build the model with a log-softmax output using `nn.LogSoftmax` or `F.log_softmax` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax)). Then you can get the actual probabilities by taking the exponential `torch.exp(output)`. With a log-softmax output, you want to use the negative log likelihood loss, `nn.NLLLoss` ([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14dc7aaa-c8dc-40c2-b523-400ad0505a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.ToTensor()\n",
    "\n",
    "train_data = datasets.MNIST(\n",
    "    'data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=20,\n",
    "    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2604bc08-4e4e-42e6-b651-41e4e8dce5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/3000\n",
      "1001/3000\n",
      "2001/3000\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for ii, (inputs, labels) in enumerate(train_loader):\n",
    "    inputs = inputs.view(inputs.shape[0], -1) # resize imagery to fit inputs\n",
    "    outputs = model(inputs) # get model output\n",
    "    loss = criterion(outputs, labels) # determine error\n",
    "\n",
    "    optimizer.zero_grad() # clear gradient from last run\n",
    "    loss.backward()       # update gradient\n",
    "    optimizer.step()      # step towards lowest level\n",
    "    \n",
    "    if ii % 1000 == 0:\n",
    "        print(f\"{ii+1}/{len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fa4ac9-bd66-4a6e-ab5c-1201a7e318f2",
   "metadata": {},
   "source": [
    "## GPU Support\n",
    "\n",
    "You can write device agnostic code which will automatically use CUDA if it's enabled like so:\n",
    "```python\n",
    "# at beginning of the script\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "...\n",
    "\n",
    "# then whenever you get a new Tensor or Module\n",
    "# this won't copy if they are already on the desired device\n",
    "input = data.to(device)\n",
    "model = MyModule(...)\n",
    "model.to(device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "411e6972-148a-41e6-92ed-031b644fd797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
