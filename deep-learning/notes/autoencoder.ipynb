{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ea16a63-d6e7-46fa-86ee-63fda849ddd3",
   "metadata": {},
   "source": [
    "![](pics/header.png)\n",
    "\n",
    "# Deep Learning: Autoencoder\n",
    "\n",
    "Kevin Walchko\n",
    "\n",
    "---\n",
    "\n",
    "These notes come from Udacity's Deep Learning Nanodegree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6395cd-9028-45e1-9cf3-0fb0e95fa670",
   "metadata": {},
   "source": [
    "## Why?\n",
    "\n",
    "Autoencoders are able to reproduce the input. \n",
    "\n",
    "![](pics/autoencoder.png)\n",
    "\n",
    "- Compression: like PCA or dimensional reduction, autoencoder can reduce the input down to the minimum amount of information needed to reconstruct it\n",
    "- Denoising: since the autoencoder understands the minimum set of information (compression above), it can remove unnecessary information (noise) and reproduce the original\n",
    "- Image reconstruction: similar to denoiseing, but instead of random additive values, it works with missing information (damage image or missing color planes) to reconstruct a full, colorized image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c993d7a-1b3b-4e76-9ab6-ec0f6a5fbfe3",
   "metadata": {},
   "source": [
    "## Error Calculation\n",
    "\n",
    "Mean squared error (MSE) is a good choice when comparing pixel quantities rather than class probabilities. \n",
    "\n",
    "```python\n",
    "criterion = nn.MSELoss()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d86bd42-7938-4f7a-880e-0a094962b633",
   "metadata": {},
   "source": [
    "## Transpose Convolution\n",
    "\n",
    "Convolution can be thought of as downsampling an image. To up sample an image, you would do transposed convolution which can be thought of as the inverse of regular convolution.\n",
    "\n",
    "![](pics/transposed-conv.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddb84b8-1baf-444a-a277-bece145dfadc",
   "metadata": {},
   "source": [
    "## Alternative to Transposed Convolution\n",
    "\n",
    "Transposed convolution can leave artifacts in yiur image. Thus, an alternative is to:\n",
    "\n",
    "1. `F.upsample(x)`\n",
    "1. `F.relu(nn.Conv2d(x))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6476345-3fdf-4f60-a05a-fb01afcc26d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "ConvAutoencoder                          --\n",
       "├─Conv2d: 1-1                            160\n",
       "├─Conv2d: 1-2                            580\n",
       "├─MaxPool2d: 1-3                         --\n",
       "├─Conv2d: 1-4                            592\n",
       "├─Conv2d: 1-5                            145\n",
       "=================================================================\n",
       "Total params: 1,477\n",
       "Trainable params: 1,477\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchinfo import summary\n",
    "\n",
    "# define the NN architecture\n",
    "class ConvAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvAutoencoder, self).__init__()\n",
    "        ## encoder layers ##\n",
    "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
    "        # conv layer (depth from 16 --> 8), 3x3 kernels\n",
    "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
    "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        ## decoder layers ##\n",
    "        self.conv4 = nn.Conv2d(4, 16, 3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(16, 1, 3, padding=1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # add layer, with relu activation function\n",
    "        # and maxpooling after\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool(x)  # compressed representation\n",
    "        \n",
    "        ## decoder \n",
    "        # upsample, followed by a conv layer, with relu activation function  \n",
    "        # this function is called `interpolate` in some PyTorch versions\n",
    "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
    "        x = F.relu(self.conv4(x))\n",
    "        # upsample again, output should have a sigmoid applied\n",
    "        x = F.upsample(x, scale_factor=2, mode='nearest')\n",
    "        x = F.sigmoid(self.conv5(x))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model = ConvAutoencoder()\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a3b6d9-b7da-4f11-a988-735cada99061",
   "metadata": {},
   "source": [
    "## Style Transfer\n",
    "\n",
    "- Gram matrix (G) contains non-localized information and contains information about the style of a given layer: \n",
    "    - given a block of feature maps (dxhxw), vectorize each feature map: 8x4x4 -> 8x16\n",
    "    - now, multiply by transpose: 8x16 x 16x8 = 8x8 = G\n",
    "    - G(1,2) contains the similarities between layer 1 and layer 2 or feature map 1 and feature map 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea3946-1ee9-446b-b891-aa4c14643798",
   "metadata": {},
   "source": [
    "## Style Loss\n",
    "\n",
    "Style loss is calculated from the MSE between the target and style gram matrices. This loss is decreased by <u>only</u> changing the Target image.\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{style} = a \\sum_i w_i (T_{s,i} - S_{s,i})^2 \\\\\n",
    "\\mathcal{L}_{content} = \\frac {1}{2} \\sum (T_c - C_c)^2 \\\\\n",
    "TotalStyleTransferLoss = \\alpha \\mathcal{L}_{content} + \\beta \\mathcal{L}_{style} \\\\\n",
    "\\alpha < \\beta \\Rightarrow \\frac {\\alpha}{\\beta}\n",
    "$$\n",
    "\n",
    "as $\\frac {\\alpha}{\\beta}$ decreases, there is less content ($\\alpha$) and more style ($\\beta$) in an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4cff80-ad9a-4e6e-b1ed-b2dda4876647",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
